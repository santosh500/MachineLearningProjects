{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import os as os\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from light_cnn import LightCNN_9Layers, LightCNN_29Layers_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root='./trainMain/',transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_data = torchvision.datasets.ImageFolder(root='./testMain/',transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mfm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, type=1):\n",
    "        super(mfm, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        if type == 1:\n",
    "            self.filter = nn.Conv2d(in_channels, 2*out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.filter = nn.Linear(in_channels, 2*out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.filter(x)\n",
    "        out = torch.split(x, self.out_channels, 1)\n",
    "        return torch.max(out[0], out[1])\n",
    "\n",
    "class group(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(group, self).__init__()\n",
    "        self.conv_a = mfm(in_channels, in_channels, 1, 1, 0)\n",
    "        self.conv   = mfm(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_a(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class resblock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(resblock, self).__init__()\n",
    "        self.conv1 = mfm(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = mfm(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out + res\n",
    "        return out\n",
    "\n",
    "\n",
    "class network_9layers(nn.Module):\n",
    "    def __init__(self, num_classes=318):\n",
    "        super(network_9layers, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            mfm(3, 48, 5, 1, 2), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True), \n",
    "            group(48, 96, 3, 1, 1), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "            group(96, 192, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True), \n",
    "            group(192, 128, 3, 1, 1),\n",
    "            group(128, 128, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "            )\n",
    "        self.fc1 = mfm(8*8*128, 256, type=0)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        out = self.fc2(x)\n",
    "        return out, x\n",
    "    \n",
    "\n",
    "class network_29layers_v2(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=79077):\n",
    "        super(network_29layers_v2, self).__init__()\n",
    "        self.conv1    = mfm(1, 48, 5, 1, 2)\n",
    "        self.block1   = self._make_layer(block, layers[0], 48, 48)\n",
    "        self.group1   = group(48, 96, 3, 1, 1)\n",
    "        self.block2   = self._make_layer(block, layers[1], 96, 96)\n",
    "        self.group2   = group(96, 192, 3, 1, 1)\n",
    "        self.block3   = self._make_layer(block, layers[2], 192, 192)\n",
    "        self.group3   = group(192, 128, 3, 1, 1)\n",
    "        self.block4   = self._make_layer(block, layers[3], 128, 128)\n",
    "        self.group4   = group(128, 128, 3, 1, 1)\n",
    "        self.fc       = nn.Linear(8*8*128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes, bias=False)\n",
    "            \n",
    "    def _make_layer(self, block, num_blocks, in_channels, out_channels):\n",
    "        layers = []\n",
    "        for i in range(0, num_blocks):\n",
    "            layers.append(block(in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.group1(x)\n",
    "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = self.block2(x)\n",
    "        x = self.group2(x)\n",
    "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = self.block3(x)\n",
    "        x = self.group3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.group4(x)\n",
    "        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        fc = self.fc(x)\n",
    "        x = F.dropout(fc, training=self.training)\n",
    "        out = self.fc2(x)\n",
    "        return out, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 128*128\n",
    "hidden_dim = 1000\n",
    "output_dim = 100\n",
    "\n",
    "model = network_9layers()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for epoch in range(20):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        print(outputs)\n",
    "        print(len(outputs))\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs[0], labels)\n",
    "        print(loss)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
